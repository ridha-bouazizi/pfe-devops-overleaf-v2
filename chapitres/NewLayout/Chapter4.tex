\graphicspath{{./assets/}}
\setcounter{mtc}{4}
\chapter{3rd Sprint: Disaster recovery }
\fancyhead[R]{\ungaramond\small\textbf{Chapter IV.   3rd Sprint :Disaster recovery }}

\minitoc
\newpage
\section*{Introduction}
Disaster recovery is a critical aspect of any IT infrastructure, and Kubernetes environments are no exception. Kubernetes provides a resilient and scalable platform for deploying and managing containerized applications, but it is still vulnerable to various types of disasters such as hardware failures, data center outages, network outages, and human error. 

Disaster recovery in a Kubernetes environment involves planning and implementing strategies to ensure that the system can recover quickly and efficiently in the event of a disaster. This includes backing up critical data, deploying application components across multiple availability zones or regions, replicating data to off-site locations, and automating failover and recovery processes. 

In this chapter, we will cover disaster recovery planning, backup and recovery strategies, data replication, and automation.  


\section{Sprint backlog :}

\begin{longtable}[H]{|m{1.5cm}|m{3cm}|m{1.5cm}|m{9cm}|}
\hline
{\textbf{Epic ID}} & {\textbf{Epic}} & {\textbf{Story ID}} & {\textbf{Story}}\\
\hline
1  & Implement a resilient disaster recovery strategy.  &  1.1	 & Provisioning cloud storage resources. \\
\cline{3-4}
& & 1.2 & Preparing and applying a backup strategy for application specific data.  \\
\cline{3-4}
& & 1.3	& Preparing and applying a backup strategy for PaaS specific workloads.   \\
\cline{3-4}
\hline
\caption{9th Sprint Backlog}
\end{longtable}

\newpage

\section{UML design: composite structure diagram of the disaster recovery strategy }

The following diagram describes the Disaster Recovery Strategy: 

\begin{figure}[H]\centering
    \includegraphics[width=1.0\textwidth,angle=00]{assets/f56.png}
    \caption{composite structure diagram of the disaster recovery strategy }
    \label{fig:f56}
\end{figure}

The disaster recovery relies on velero, a Kubernetes aware backup tool, and proprietary backup and restore containers written in python. 

Overall, this composite structure diagram shows how Velero and Kubernetes cronjobs work together to implement a disaster recovery strategy on Kubernetes that includes backing up and restoring both Kubernetes objects and database cluster data. 

\section{Disaster recovery for the PaaS environment }

As shown above in the composite structure diagram, velero is a powerful tool that enables us to backup and restore entire Kubernetes namespaces. 

\subsection{Provider specific configurations }

Provisioning the object store containers that are destined to contain and serve backup data is dependent on the cloud provider.

To align with our strategy, we have developed an ansible playbook encompassing multiple roles. The first role focuses on installing essential Python packages, configuring OpenStack and AWS CLI settings, and facilitating interactions with the S3 api to handle credentials and access bucket lists.

The second role concentrates on installing and configuring the Velero client on master nodes, implementing CRDs and RBAC configurations, encoding AWS credentials, and deploying the Velero client to the Kubernetes cluster. This playbook serves our strategic objectives efficiently.

 \begin{figure}[H]\centering
\includegraphics[width=0.95\textwidth,angle=00]{assets/f57.png}
\caption{Architecture of kubernetes objects Backup }
\label{fig:f57}
\end{figure}

The backup and restore operations of the various Kubernetes objects are initiated by the operator either manually or using an ansible playbook tailored specifically for our PaaS. 

As illustrated in the architecture above, the deployed Velero controller communicates with the Kubernetes API server to identify resources that need to be backed up or restored. In case of a backup operation, it stores backup data in an object store. This data is then retrieved to be restored. 

\section{Disaster recovery for the Database servers }

Only backing up persisted Kubernetes objects is unfortunately insufficient. In Kubernetes, backing up volumes that contain a database's data is not recommended because it can lead to inconsistent data being backed up.

When a backup of a volume is created while the database is still running and actively writing to the volume, the backup might include an inconsistent view of the data. This is because the backup process is not transactionally consistent with the database, meaning that the backup might capture changes that were in progress but not yet completed. To avoid this issue, it is recommended to dump the database data first. 

 \subsection{Programmatically backing up and restoring database data }
\subsubsection{Automating database backups}
In order to eliminate the need for user intervention, we have developed a few python programs that are charged to efficiently perform backup and restore operations. 

The following class diagram illustrates a Python-based backup agent that is designed to ensure the secure and efficient backup of replicated MongoDB cluster data.

With its robust functionality and reliability, this agent simplifies the process of creating backups and ensures the integrity of data.

Developed specifically for MongoDB clusters, this backup agent provides a seamless and streamlined solution for safeguarding valuable data assets.

The program collects MongoDB configuration parameters, such as the database host, port, username, and password, from environment variables or configuration files. 

 \begin{figure}[H]\centering
\includegraphics[width=1.0\textwidth,angle=00]{assets/f58.png}
\caption{Backup agent for the replicated MongoDB }
\label{fig:f58}
\end{figure}

The program defines several classes, each of which has methods that perform specific functions: 

\begin{itemize}[label={--}]
\item “NiceLogger”: A logging utility that logs messages with timestamps. 
\item “BackupCleaner”: A class that contains a method to clean up old backups from the persistent volume native to the PaaS environment. 
\item  “CompressBackup”: A class that contains a method to compress the backup  data. 
\item  “S3Uploader”: A class that contains a method to upload the compressed backup to AWS S3 cloud storage. 
\item “MinioUploader”: A class that contains a method to upload the compressed backup to MinIO cloud storage. 
\item  “Notifier”: A class that sends a notification message to Microsoft Teams. 
\item  “CommandRunner”: A class that runs shell commands. 
\item “MongoDumper”: A class that contains a method to create a MongoDB dump, compress it, and upload it to cloud storage. 
\end{itemize}
 
In a similar fashion to backing up the MongoDB cluster data, we have developed a backup agent for PostgreSQL. PostgreSQL and MongoDB have different data storage architectures.  

When backing up PostgreSQL, it's important to back up both the data and the schema because the schema defines the structure of the tables and the relationships between them. Without the schema, it would be difficult to restore the database to its original state.

\subsubsection{Streamlining database data restores}

An intuitive way of implementing database restore operations is reversing the backup process. Data is first fetched from remote object stores, decrypted, decompressed and then reinjected the cluster of database servers. The following class diagram show the architecture of the restore agent in the case of mongodb.

\begin{figure}[H]\centering
\includegraphics[width=1.0\textwidth,angle=00]{assets/f60.png}
\caption{Restorer agent for the replicated MongoDB }
\label{fig:f60}
\end{figure}

\subsection{ Scheduling backup and restore jobs }

For each database cluster we have built a container image to which we then inject the python program responsible for backing up or restoring data. This is then used to configure a Kubernetes “CronJob” that runs periodically.

These are some of the specifications we are adding to the cronjob object declaration: 
\begin{itemize}[label={--}]
\item “schedule: ‘0 0 4,12,20,28 * *’ ”: This defines the time and frequency at which the job will run, using a cron expression. In this case, the job will run at midnight, every 8 days of each month. 
\item  “concurrencyPolicy: Forbid”: This specifies how to handle overlapping job executions. With the "Forbid" option, if a job is already running at the time a new one is scheduled to start, the new job will not be run. 
\end{itemize}

Other than the environment variables to configure the backup program and the volume mounts to inject config files, we have attached a Persistent Volume from our distributed storage cluster to be able to backup data locally. 

Each of the python programs we have developed can back up and restore the entirety of the cluster data in one go. It only amounts to Setting the “DB\_NAME” environment variable to “NULL”. 

\newpage

\section*{Conclusion}
Disaster recovery is a critical aspect of any PaaS environment that hosts mission-critical applications and services. The ability to recover from disasters quickly and efficiently can make the difference between a minor service disruption and a significant business failure. 

For Kubernetes-based PaaS environments, disaster recovery planning should include considerations such as data replication and backup, failover strategies, and hosting location redundancy.  

Similarly, for MongoDB and PostgreSQL database servers, disaster recovery planning should involve robust backup and restoration processes, as well as monitoring and testing procedures to identify and address potential points of failure.  

Ultimately, disaster recovery planning is an ongoing process that requires continuous monitoring and refinement. 